# 챗봇 구동 방식

## 전체 아키텍처

```
[사용자] → [Next.js 프론트엔드] → [Next.js API Route (프록시)] → [FastAPI 봇 서버] → [OpenAI API]
                                                                        ↕
                                                                  [ChromaDB (벡터 DB)]
                                                                        ↕
                                                                  [knowledge/ 폴더 (문서)]
```

### 구성 요소

| 구성 요소 | 기술 스택 | 역할 |
|-----------|----------|------|
| 프론트엔드 UI | Next.js + ChatBot 컴포넌트 | 사용자 채팅 인터페이스 |
| API 프록시 | Next.js API Route (`/api/chat`) | 봇 서버로의 요청 중계 |
| 봇 서버 | FastAPI (Python) | RAG 파이프라인 실행, API 제공 |
| 벡터 DB | ChromaDB | 문서 임베딩 저장 및 유사도 검색 |
| LLM | OpenAI GPT-4o-mini | 답변 생성 |
| 임베딩 | OpenAI text-embedding-3-small | 문서/질문 벡터화 |

---

## 요청 흐름

### 1. 사용자 질문 전송

사용자가 채팅 UI에서 질문을 입력하면 프론트엔드가 Next.js API Route로 요청을 보냅니다.

```
POST /api/chat
Body: { "question": "오픈런이 뭐야?" }
```

### 2. API 프록시 (Next.js → FastAPI)

`frontend/src/app/api/chat/route.ts`가 봇 서버의 RAG 엔드포인트로 요청을 중계합니다.

```
POST {BOT_SERVER_URL}/rag/query
Body: { "question": "오픈런이 뭐야?" }
```

- `BOT_SERVER_URL`은 프론트엔드의 환경변수로 설정
- 프록시를 사용하는 이유: CORS 우회, API 키 보호, 서버 주소 은닉

### 3. RAG 파이프라인 (봇 서버)

봇 서버에서 RAG(Retrieval-Augmented Generation) 방식으로 답변을 생성합니다.

**단계:**

1. **문서 검색 (Retrieval)**: 질문을 임베딩하여 ChromaDB에서 가장 관련 있는 문서 청크 3개를 검색
2. **프롬프트 구성 (Augmentation)**: 검색된 문서를 시스템 프롬프트에 삽입
3. **답변 생성 (Generation)**: GPT-4o-mini가 문서 기반으로 답변 생성

### 4. 응답 반환

```json
{
  "answer": "오픈런은 함께 달리는 러닝 소셜 서비스입니다...",
  "sources": [
    { "content": "문서 내용...", "source": "01_서비스_개요.md" }
  ]
}
```

---

## API 엔드포인트

| 메서드 | 경로 | 설명 |
|--------|------|------|
| GET | `/` | 서버 상태 확인 |
| POST | `/rag/query` | RAG 기반 질문 답변 (프론트엔드에서 사용) |
| POST | `/rag/locate` | 문서 위치 검색 (관리 UI 챗봇에서 사용) |
| POST | `/chat` | 일반 GPT 채팅 (RAG 미사용) |
| GET | `/models` | 사용 가능한 GPT 모델 목록 |
| GET | `/docs` | Swagger UI (FastAPI 자동 생성) |

> 문서 관리 관련 엔드포인트(`/documents/*`, `/manage`)는 [문서_관리_프로세스.md](문서_관리_프로세스.md)를 참고하세요.

---

## 실행 방법

### 로컬 실행

```bash
cd bot
source .venv/bin/activate
python main.py
```

서버가 `http://localhost:8000`에서 실행됩니다.

### Docker 실행

```bash
cd bot
docker-compose up --build
```

- `knowledge/` 폴더가 볼륨 마운트되어 로컬 파일 변경이 실시간 반영됩니다.
- 포트: `8000:8000`

### 프론트엔드 연동

프론트엔드의 `.env`에 봇 서버 주소를 설정합니다:

```env
BOT_SERVER_URL=http://localhost:8000
```

---

## 기술 상세

### 사용 라이브러리

| 라이브러리 | 버전 | 용도 |
|-----------|------|------|
| FastAPI | ≥0.115.0 | 웹 서버 프레임워크 |
| uvicorn | ≥0.34.0 | ASGI 서버 |
| openai | ≥1.0.0 | OpenAI API 클라이언트 |
| langchain | ≥0.3.0 | RAG 파이프라인 |
| langchain-openai | ≥0.3.0 | LangChain OpenAI 연동 |
| langchain-chroma | ≥0.2.0 | LangChain ChromaDB 연동 |
| chromadb | ≥0.6.0 | 벡터 데이터베이스 |
| watchfiles | - | 파일 변경 감시 (watcher.py에서 사용) |
| python-dotenv | ≥1.0.0 | 환경변수 관리 |

### 디렉토리 구조

```
bot/
├── main.py              # FastAPI 서버 진입점
├── schemas.py           # Pydantic 요청/응답 모델
├── requirements.txt     # Python 의존성
├── Dockerfile           # Docker 이미지 정의
├── docker-compose.yml   # Docker Compose 설정
├── .env                 # 환경변수 (OPENAI_API_KEY)
├── rag/                 # RAG 파이프라인 모듈
│   ├── chain.py         # LangChain RAG 체인 구성
│   ├── document_loader.py  # 문서 로드 및 청크 분할
│   ├── vector_store.py  # ChromaDB 벡터 저장소 관리
│   └── watcher.py       # knowledge/ 폴더 실시간 감시
├── knowledge/           # RAG 참조 문서 (마크다운)
├── chroma_db/           # ChromaDB 데이터 (자동 생성, gitignore)
├── static/              # 관리 UI 정적 파일
│   └── manage.html      # 관리 페이지
├── scripts/             # 유틸리티 스크립트
│   └── generate_docs.py
└── docs/                # 프로젝트 문서
    ├── README.md            # 문서 인덱스
    ├── 서버_실행_가이드.md
    ├── 구동_방식.md      # (이 문서)
    └── 문서_관리_프로세스.md  # 문서 관리 방법 및 CI/CD
```
